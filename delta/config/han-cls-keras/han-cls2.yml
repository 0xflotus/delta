---
# han in kuai_che_2 data set
data:
  train:
    paths:
      - /nfs/project/datasets/all_data/data_kuai_che/kc.train_0315.shuf.std
  dev:
    paths:
      - /nfs/project/datasets/all_data/data_kuai_che/kc.test_0315.shuf.std
  eval:
    paths:
      - /nfs/project/datasets/all_data/data_kuai_che/kc.test_0315.shuf.std
  infer:
    paths:
      - /nfs/project/datasets/all_data/data_kuai_che/kc.test_0315.shuf.std
    res: ckpt/han-cls/res/infer.txt
  task:
    name: TextClsTask
    use_dense: false
    language: chinese
    split_by_space: true
    keyword: ckpt/han-cls/data/keyword_list
    vocab_min_frequency: 20
    text_vocab: ckpt/han-cls/data/text_vocab.txt
    label_vocab: ckpt/han-cls/data/label_vocab.txt
    max_seq_len: 1024
    batch_size: 32
    epochs: 30
    num_parallel_calls: 12
    num_prefetch_batch: 2
    shuffle_buffer_size: 200000
    need_shuffle: true
    classes:
      num_classes: 4
      positive_id: 1
      vocab:
        0: 0
        1: 1
        2: 2
        3: 3

model:
  name: HierarchicalAttentionModel
  type: keras  # raw, keras or eager model
  use_pre_train_emb: true
  pre_train_emb_path: /nfs/project/datasets/all_data/data_w2v/w2v_char.txt
  embedding_path: ckpt/han-cls/data/embeding.pkl
  net:
    structure:
      embedding_size: 200
      emb_trainable: true
      cell_type: cudnngru # gru/lstm/cudnngru/cudnnlstm
      cell_dim: 100
      num_layers: 1
      max_len: 1024
      max_sen_len: 32
      max_doc_len: 32
      dropout_rate: 0.5
      l2_reg_lambda: 0

solver:
  name: RawSolver
  quantization:
    enable: false # whether to quantization model
    quant_delay: 0 # Number of steps after which weights and activations are quantized during training
  adversarial:
    enable: false # whether to using adversiral training
    adv_alpha: 0.5 # adviseral alpha of loss
    adv_epslion: 0.1 # adviseral example epslion
  model_average:
    enable: false # use average model
    var_avg_decay: 0.99 # the decay rate of varaibles
  optimizer:
    name: rmsprop
    loss: CrossEntropyLoss 
    label_smoothing: 0.0 # label smoothing rate
    learning_rate:
      rate: 0.001 # learning rate of Adam optimizer
      type:  exp_decay # learning rate type
      decay_rate: 0.99  # the lr decay rate
      decay_steps: 100  # the lr decay_step for optimizer
    clip_global_norm: 3.0 # clip global norm
    multitask: False # whether is multi-task
  metrics:
    pos_label: 1 # int, same to sklearn
    cals:
      - name: AccuracyCal
        arguments: Null
      - name: PrecisionCal
        arguments:
          average: 'macro'
      - name: RecallCal
        arguments:
          average: 'macro'
      - name: F1ScoreCal
        arguments:
          average: 'weighted'
    keras:
      - name: acc
  saver:
    model_path: "ckpt/han-cls"
    max_to_keep: 10
    save_checkpoint_steps: 100
    print_every: 10
    resume_model_path: ""
  service:
    model_path: ckpt/han-cls/saved_model
    pb_path: ckpt/han-cls/pb/freezed.pb
    model_version: "1"
  run_config:
    tf_random_seed: null
    allow_soft_placement: true
    log_device_placement: false
    intra_op_parallelism_threads: 10
    inter_op_parallelism_threads: 10
    allow_growth: true
